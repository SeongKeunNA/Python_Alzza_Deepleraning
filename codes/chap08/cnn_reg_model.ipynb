{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d540b09d",
   "metadata": {},
   "source": [
    "# 8.8 구현하기 : 정규화 학장 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d89be3",
   "metadata": {},
   "source": [
    "## 8.8.1 기반 클래스 파일 실행시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f81890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../chap07/cnn_basic_model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e24a1",
   "metadata": {},
   "source": [
    "## 8.8.2 CnnRegModel 클래스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cfc268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnRegModel(CnnBasicModel):\n",
    "    def __init__(self, name, dataset, hconfigs, show_maps=False,\n",
    "                l2_decay=0, l1_decay=0):\n",
    "        self.l2_decay = l2_decay\n",
    "        self.l1_decay = l1_decay\n",
    "        super(CnnRegModel, self).__init__(name, dataset, hconfigs, show_maps)\n",
    "        \n",
    "    def exec_all(self, epoch_count=10, batch_size=10, learning_rate=0.001,\n",
    "                 report=0, show_cnt=3, show_params=False):\n",
    "        super(CnnRegModel, self).exec_all(epoch_count, batch_size,\n",
    "                                          learning_rate, report, show_cnt)\n",
    "        if show_params: self.show_param_dist()          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0764f51",
   "metadata": {},
   "source": [
    "## 8.8.3 파라미터 분포 출력 메서드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71fd8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_reg_show_param_dist(self):\n",
    "    params = self.collect_params()\n",
    "    mu = np.mean(params)\n",
    "    sigma = np.sqrt(np.var(params))\n",
    "    plt.hist(params, 100, density=True, facecolor='g', alpha=0.75)\n",
    "    plt.axis([-0.2, 0.2, 0, 20.0])\n",
    "    plt.text(0.08, 15.0, 'mu={:5.3f}'.format(mu))\n",
    "    plt.text(0.08, 13.0, 'sigma={:5.3f}'.format(sigma))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    total_count = len(params)\n",
    "    near_zero_count = len(list(x for x in params if -1e-5 <= x <= 1e-5))\n",
    "    print('Near 0 parameters = {:4.1f}%({}/{})'.\n",
    "         format(near_zero_count/total_count*100, near_zero_count, total_count))\n",
    "    \n",
    "def cnn_reg_collect_params(self):\n",
    "    params = list(self.pm_output['w'].flatten())\n",
    "    for pm in self.pm_hiddens:\n",
    "        if 'w' in pm: params += list(pm['w'].flatten())\n",
    "        if 'k' in pm: params += list(pm['k'].flatten())\n",
    "    return params\n",
    "\n",
    "CnnRegModel.show_param_dist = cnn_reg_show_param_dist\n",
    "CnnRegModel.collect_params = cnn_reg_collect_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88da96",
   "metadata": {},
   "source": [
    "## 8.8.4 추가 손실 함수 계산 매서드 재정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422892ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_reg_forward_extra_cost(self, y):\n",
    "    extra, aux_extra = super(CnnRegModel, self).forward_extra_cost(y)\n",
    "    if self.l2_decay > 0 or self.l1_decay > 0:\n",
    "        params = self.collect_params()\n",
    "        if self.l2_decay > 0:\n",
    "            extra += np.sum(np.square(params)) / 2\n",
    "        if self.l1_decay > 0:\n",
    "            extra += np.sum(np.abs(params))\n",
    "    return extra, aux_extra\n",
    "                                            # 평가 단계에서는 불필요하지만 이 메서드는 손실 함수 계산 과정에서 호출되며 손실 함수,\n",
    "                                            # 계산 과정은 원래 학습 단계에서만 호출되므로 self.is_training 플래그 값 검사는 불필요함\n",
    "CnnRegModel.forward_extra_cost = cnn_reg_forward_extra_cost "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c4b70",
   "metadata": {},
   "source": [
    "## 8.8.5 파라미터 갱신 함수 재정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de29264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_reg_update_param(self, pm, key, delta):\n",
    "    if self.use_adam:\n",
    "        delta = self.eval_adam_delta(pm, key, delta)\n",
    "\n",
    "    if key in ['w', 'k']:\n",
    "        if self.l2_decay > 0:\n",
    "            delta += self.l2_decay * pm[key]\n",
    "        if self.l1_decay > 0:\n",
    "            delta += self.l1_decay * np.sign(pm[key])\n",
    "            \n",
    "    pm[key] -= self.learning_rate * delta\n",
    "\n",
    "CnnRegModel.update_param = cnn_reg_update_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf575f4",
   "metadata": {},
   "source": [
    "## 8.8.6 드롭아웃 계층 지원을 위한 세 가지 메서드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac4636a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_reg_alloc_dropout_layer(self, input_shape, hconfig):\n",
    "    keep_prob = get_conf_param(hconfig, 'keep_prob', 1.0)\n",
    "    assert 0 < keep_prob <= 1\n",
    "    return {'keep_prob': keep_prob}, input_shape\n",
    "\n",
    "def cnn_reg_forward_dropout_layer(self, x, hconfig, pm):\n",
    "    if self.is_training:\n",
    "        dmask = np.random.binomial(1, pm['keep_prob'], x.shape) # x.shape[1:]로 미니배치 단위로 일괄처리해야 효과적이라함(?)\n",
    "        dropped = x * dmask / pm['keep_prob'] # 기댓값을 유지시키기 위해 남은 파라미터를 pm['keep_prob']의 역수만큼 증폭시킴\n",
    "        return dropped, dmask\n",
    "    else:\n",
    "        return x, None\n",
    "def cnn_reg_backprop_dropout_layer(self, G_y, hconfig, pm, aux):\n",
    "    dmask = aux\n",
    "    G_hidden = G_y * dmask / pm['keep_prob'] # 마스크를 곱해 사라진(0이된) 값을 곱해 0으로 만듬, 순전파와 마찬가지로 기울기 값도 증폭시킴\n",
    "    return G_hidden\n",
    "\n",
    "CnnRegModel.alloc_dropout_layer = cnn_reg_alloc_dropout_layer\n",
    "CnnRegModel.forward_dropout_layer = cnn_reg_forward_dropout_layer\n",
    "CnnRegModel.backprop_dropout_layer = cnn_reg_backprop_dropout_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9db3a9",
   "metadata": {},
   "source": [
    "## 8.8.7 잡음 주입 계층 지원을 위한 세 가지 메서드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09f12bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_reg_alloc_noise_layer(self, input_shape, hconfig):\n",
    "    noise_type = get_conf_param(hconfig, 'type', 'normal')\n",
    "    mean = get_conf_param(hconfig, 'mean', 0)\n",
    "    std = get_conf_param(hconfig, 'std', 1.0)\n",
    "    ratio = get_conf_param(hconfig, 'ratio', 1.0) # 잡음 주입 비율\n",
    "    \n",
    "    assert noise_type == 'normal' # 여기에서는 정규분포로 제한\n",
    "    \n",
    "    return {'mean':mean, 'std':std, 'ratio':ratio}, input_shape\n",
    "\n",
    "def cnn_reg_forward_noise_layer(self, x, hconfig, pm):\n",
    "    if self.is_training and np.random.rand() < pm['ratio']:\n",
    "        noise = np.random.normal(pm['mean'], pm['std'], x.shape)\n",
    "        return x + noise, None\n",
    "    else:\n",
    "        return x, None\n",
    "    \n",
    "def cnn_reg_backprop_noise_layer(self, G_y, hconfig, pm, aux):\n",
    "    return G_y # 아무 일도 할 필요 없이 메서드 자체만 정의\n",
    "\n",
    "CnnRegModel.alloc_noise_layer = cnn_reg_alloc_noise_layer\n",
    "CnnRegModel.forward_noise_layer = cnn_reg_forward_noise_layer\n",
    "CnnRegModel.backprop_noise_layer = cnn_reg_backprop_noise_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7974065",
   "metadata": {},
   "source": [
    "## 8.8.8 배치 정규화 계층 지원을 위한 세 가지 메서드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1722ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_reg_alloc_batch_normal_layer(self, input_shape, hconfig):\n",
    "    pm = {}\n",
    "    rescale = get_conf_param(hconfig, 'rescale', True)\n",
    "    pm['epsilon'] = get_conf_param(hconfig, 'epsilon', 1e-10)\n",
    "    pm['exp_ratio'] = get_conf_param(hconfig, 'exp_ratio', 0.001)\n",
    "    \n",
    "    bn_dim = input_shape[-1] # 배치 정규화 그룹 수. 완전 연결 계층의 경우 미니배치 데이터의 입력 벡터 크기, 합성곱 계층의 경우 출력 채널수\n",
    "    pm['mavg'] = np.zeros(bn_dim)\n",
    "    pm['mvar'] = np.ones(bn_dim)\n",
    "    if rescale: # 이 두 가지는 학습 대상 파라미터\n",
    "        pm['scale'] = np.ones(bn_dim) \n",
    "        pm['shift'] = np.zeros(bn_dim)\n",
    "    return pm, input_shape\n",
    "\n",
    "def cnn_reg_forward_batch_normal_layer(self, x, hconfig, pm):\n",
    "    if self.is_training:\n",
    "        x_flat = x.reshape([-1, x.shape[-1]]) # 마지막 차원을 별로 일렬로 폄\n",
    "        avg = np.mean(x_flat, axis=0)\n",
    "        var = np.var(x_flat, axis=0)\n",
    "        pm['mavg'] += pm['exp_ratio'] * (avg - pm['mavg']) # 평균의 이동 평균을 구함\n",
    "        pm['mvar'] += pm['exp_ratio'] * (var - pm['mvar']) # 분산의 이동 평균을 구함\n",
    "    else:                             # 구해놓은 이동 평균을 사용함\n",
    "        avg = pm['mavg'] \n",
    "        var = pm['mvar'] \n",
    "    std = np.sqrt(var + pm['epsilon']) # 0으로 나누기 방지\n",
    "    y = norm_x = (x - avg) / std\n",
    "    if 'scale' in pm:\n",
    "        y = pm['scale'] * norm_x + pm['shift'] \n",
    "    return y, [norm_x, std] #역전파 보조 정보로 norm_x 필요\n",
    "\n",
    "def cnn_reg_backprop_batch_normal_layer(self, G_y, hconfig, pm, aux):\n",
    "    norm_x, std = aux\n",
    "    if 'scale' in pm: # n * m  m* k n *k\n",
    "        if len(G_y.shape) == 2: axis = 0 # MLP(2차원)의 경우 미니배치 데이터 내에서 더함\n",
    "        else: axis = (0,1,2) # CNN(4차원)의 경우 채널을 제외한 앞의 3차원 별로 모두 더함\n",
    "        G_scale = np.sum(G_y*norm_x, axis = axis)\n",
    "        G_shift = np.sum(G_y, axis=axis)\n",
    "        G_y = G_y * pm['scale'] \n",
    "        pm['scale'] -= self.learning_rate * G_scale\n",
    "        pm['shift'] -= self.learning_rate * G_shift\n",
    "    G_input = G_y / std # 순전파 처럼 std로 나눠줌\n",
    "    return G_input\n",
    "\n",
    "CnnRegModel.alloc_batch_normal_layer = cnn_reg_alloc_batch_normal_layer\n",
    "CnnRegModel.forward_batch_normal_layer = cnn_reg_forward_batch_normal_layer\n",
    "CnnRegModel.backprop_batch_normal_layer = cnn_reg_backprop_batch_normal_layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
